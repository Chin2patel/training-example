{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72569884-01ed-4efb-b6a9-358bbd4f73ee",
   "metadata": {},
   "source": [
    "# Cheap Talk and Cherry-Picking: What ClimateBert has to say on Corporate Climate Risk Disclosures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cce4e-313f-4c53-b7e5-c59bfb8412f7",
   "metadata": {},
   "source": [
    "### Julia Anna Bingler, Mathias Kraus, Markus Leippold, Nicolas Webersinke\n",
    "\n",
    "### DOI:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1b97c-3405-4ef5-90f4-001a7b16bfa5",
   "metadata": {},
   "source": [
    "This notebook provides an example of the model training from our paper. It can be used as a reference for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14316d96-0b4f-48e3-912f-5a175bdffe06",
   "metadata": {},
   "source": [
    "#### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0c64c-6145-422c-a75d-e523bcc346b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eec502-7a2e-4eed-821b-cbc35434ab64",
   "metadata": {},
   "source": [
    "#### Define labels and set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c86d15-8eff-4f86-b909-3d415c226917",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "label_to_id = {\n",
    "    'Governance': 0,\n",
    "    'Metrics and Targets': 1,\n",
    "    'Risk Management': 2,\n",
    "    'Strategy': 3,\n",
    "    'None': 4\n",
    "}\n",
    "\n",
    "id_to_label = {\n",
    "    0: 'Governance',\n",
    "    1: 'Metrics and Targets',\n",
    "    2: 'Risk Management',\n",
    "    3: 'Strategy',\n",
    "    4: 'None'\n",
    "}\n",
    "\n",
    "np.random.seed(0)\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac332e7-ffd9-41a0-8700-9b4d261c45d9",
   "metadata": {},
   "source": [
    "#### Define a custom pytorch dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0280b93-85bb-4891-9b0b-03c2555b40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCFDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761e513-7283-4e34-9cfe-42cc0aa8cfb5",
   "metadata": {},
   "source": [
    "#### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9270e-e5a7-422e-9e45-b23d6a052572",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "companies = []\n",
    "\n",
    "with open(\"training_data.json\", \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "\n",
    "    for sample in training_data:\n",
    "        texts.append(sample[\"text\"])\n",
    "        labels.append(label_to_id[sample[\"label\"]])\n",
    "        companies.append(sample[\"company\"])\n",
    "\n",
    "assert len(texts) == len(labels) == len(companies)\n",
    "\n",
    "texts = np.array(texts)\n",
    "labels = np.array(labels)\n",
    "companies = np.array(companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f3f6f-3a59-4bd3-81f4-7d83dd7e48ab",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ee75f-bee1-46f6-8e13-32ef4d60e6df",
   "metadata": {},
   "source": [
    "This cell includes the actual training loop. Adjust paths and parameters for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804ace3-45df-40b3-91a7-dce3010b57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = np.unique(companies)\n",
    "np.random.shuffle(comp_list)\n",
    "\n",
    "for comp_i in np.arange(len(comp_list))[::10]:\n",
    "    comp_l = comp_list[comp_i:comp_i+10]\n",
    "\n",
    "    print(comp_l)\n",
    "\n",
    "    #shutil.rmtree('results', ignore_errors=True)\n",
    "\n",
    "    comp_texts = texts[np.isin(companies, comp_l)]\n",
    "    comp_labels = labels[np.isin(companies, comp_l)]\n",
    "    comp_comps = companies[np.isin(companies, comp_l)]\n",
    "\n",
    "    non_comp_texts = texts[~np.isin(companies, comp_l)]\n",
    "    non_comp_labels = labels[~np.isin(companies, comp_l)]\n",
    "\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(non_comp_texts, non_comp_labels, test_size=.2, stratify=non_comp_labels, random_state=0)\n",
    "\n",
    "    print('Train samples: {}'.format(len(train_texts)))\n",
    "    print('Validation samples: {}'.format(len(val_texts)))\n",
    "    print('Test samples: {}'.format(len(comp_texts)))\n",
    "\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained('distilroberta-base')\n",
    "\n",
    "    train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = TCFDDataset(train_encodings, train_labels)\n",
    "    val_dataset = TCFDDataset(val_encodings, val_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "       output_dir='./results',          # output directory\n",
    "       overwrite_output_dir=True,\n",
    "       num_train_epochs=10,             # total number of training epochs\n",
    "       per_device_train_batch_size=24,  # batch size per device during training\n",
    "       per_device_eval_batch_size=24,   # batch size for evaluation\n",
    "       warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "       weight_decay=0.01,               # strength of weight decay\n",
    "       logging_dir='./logs',            # directory for storing logs\n",
    "       logging_steps=10,\n",
    "       fp16=True,                       # enable mixed precision training if supported by GPU\n",
    "       gradient_accumulation_steps=4,\n",
    "       load_best_model_at_end=True,\n",
    "       evaluation_strategy='epoch',\n",
    "       save_strategy='epoch'\n",
    "    )\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"distilroberta-base\",\n",
    "                                                            num_labels=num_classes)\n",
    "\n",
    "    early_stop = EarlyStoppingCallback(2)\n",
    "\n",
    "    trainer = Trainer(\n",
    "       model=model,\n",
    "       args=training_args,\n",
    "       train_dataset=train_dataset,\n",
    "       eval_dataset=val_dataset,\n",
    "       callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    for comp in np.unique(comp_comps):\n",
    "        for label in np.unique(labels[companies == comp]):\n",
    "\n",
    "            comp_label_text = comp_texts[(comp_comps == comp) & (comp_labels == label)]\n",
    "            comp_label_labels = comp_labels[(comp_comps == comp) & (comp_labels == label)]\n",
    "            test_encodings = tokenizer(list(comp_label_text), truncation=True, padding=True)\n",
    "            test_dataset = TCFDDataset(test_encodings, comp_label_labels)\n",
    "\n",
    "\n",
    "            x = trainer.predict(test_dataset)[0]\n",
    "        \n",
    "            with open('output.txt', 'a', encoding='utf-8') as fd:\n",
    "                for i, sent in enumerate(comp_label_text):\n",
    "                    fd.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(comp, label, x[i,0], x[i,1], x[i,2], x[i,3], x[i, 4]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd459ad9-5046-4e0d-8c6d-ae94a395db28",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3756702-2740-4a83-9194-12ed688f337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.txt', sep='\\t', header=None, names=['comp', 'label', 'pred_0',\n",
    "                                                          'pred_1', 'pred_2', 'pred_3',\n",
    "                                                          'pred_4'])\n",
    "\n",
    "# df = df[df[['pred_0', 'pred_1', 'pred_2', 'pred_3', 'pred_4']].max(axis=1) > 1.]\n",
    "\n",
    "df['pred_class'] = np.argmax(df[['pred_0', 'pred_1', 'pred_2', 'pred_3', 'pred_4']].values, axis=1)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for comp in df.comp.unique():\n",
    "    df_tmp = df[df.comp == comp]\n",
    "    for lab in [0, 1, 2, 3, 4]:\n",
    "        if len(df_tmp[df_tmp.label == lab]) == 0:\n",
    "            continue\n",
    "        df_comp_tmp = df_tmp[df_tmp.label == lab]\n",
    "        x = [np.sum(df_comp_tmp['pred_class'] == 0) / len(df_comp_tmp['pred_class']),\n",
    "              np.sum(df_comp_tmp['pred_class'] == 1) / len(df_comp_tmp['pred_class']),\n",
    "              np.sum(df_comp_tmp['pred_class'] == 2) / len(df_comp_tmp['pred_class']),\n",
    "              np.sum(df_comp_tmp['pred_class'] == 3) / len(df_comp_tmp['pred_class'])]#,\n",
    "              #np.sum(df_comp_tmp['pred_class'] == 4) / len(df_comp_tmp['pred_class'])]\n",
    "        X.append(x)\n",
    "        y.append(lab)\n",
    "        \n",
    "clf = LogisticRegression(penalty='none')\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.coef_)\n",
    "pred = clf.predict(X)\n",
    "\n",
    "cm = confusion_matrix(y, pred)\n",
    "print(cm)\n",
    "\n",
    "for y_select in np.unique(y):\n",
    "    print(cm[y_select, y_select] / np.sum(cm[:,y_select]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_training]",
   "language": "python",
   "name": "conda-env-nlp_training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
